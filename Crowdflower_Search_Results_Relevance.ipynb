{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crowdflower Search Results Relevance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJfIL_QB-WBD",
        "colab_type": "text"
      },
      "source": [
        "# Environment set up and data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbX78BvBEdOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kaggle --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gecgXh9aE1oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing the enecessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "import os\n",
        "import zipfile\n",
        "import torchvision.models as tvm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as PACK\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "import urllib.request\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emp-4hxxFFr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Upload the kaggle .json file when prompted\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7arvtFSG0jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing the data from the website\n",
        "\n",
        "! mkdir -p ~/.kaggle/\n",
        "! mv kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! mkdir -p /content/crowdflower-problem\n",
        "% cd /content/crowdflower-problem\n",
        "\n",
        "!kaggle competitions download -c crowdflower-search-relevance\n",
        "\n",
        "## Downloading the word2vec file\n",
        "\n",
        "!wget -P /content -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "w2vmodel = gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin.gz', binary=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LGh-nPaFbqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Unzipping the data\n",
        "\n",
        "with zipfile.ZipFile(\"/content/crowdflower-problem/train.csv.zip\", 'r') as zip_ref:\n",
        "  zip_ref.extractall(\"/content/crowdflower-problem/train\")\n",
        "\n",
        "with zipfile.ZipFile(\"/content/crowdflower-problem/test.csv.zip\", 'r') as zip_ref:\n",
        "  zip_ref.extractall(\"/content/crowdflower-problem/test\")\n",
        "\n",
        "os.remove(\"test.csv.zip\")\n",
        "os.remove(\"train.csv.zip\")\n",
        "\n",
        "train_csv = pd.read_csv(\"/content/crowdflower-problem/train/train.csv\",engine='python')\n",
        "test_csv = pd.read_csv(\"/content/crowdflower-problem/test/test.csv\",engine='python')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNcvl1LWgUqv",
        "colab_type": "text"
      },
      "source": [
        "#Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQRfOuIQZPIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Data cleaning function\n",
        "\n",
        "def data_cleaning(data_csv):\n",
        "\n",
        "  ## Replace Nan with blank spaces\n",
        "\n",
        "  data_csv.loc[data_csv['product_description'].isnull(),'product_description'] = data_csv['product_title']\n",
        "  ### lowercasing the tokens in columns\n",
        "  data_csv['product_title'] = data_csv['product_title'].str.replace('\\W', ' ')\n",
        "  data_csv['product_description'] = data_csv['product_description'].str.replace('\\W', ' ')\n",
        "\n",
        "  data_csv['query_cleaned'] = data_csv['query'].str.lower().str.split()\n",
        "  data_csv['product_title_cleaned'] = data_csv['product_title'].str.lower().str.split()\n",
        "  \n",
        "  data_csv['product_description_cleaned'] = data_csv['product_description'].str.lower().str.split()\n",
        "  \n",
        "\n",
        "  ### Removing stopwords\n",
        "  data_csv['product_description_cleaned'] = data_csv['product_description_cleaned'].apply(lambda x: [item for item in x if item not in stop])\n",
        "  data_csv['product_title_cleaned'] = data_csv['product_title_cleaned'].apply(lambda x: [item for item in x if item not in stop])\n",
        "  return data_csv\n",
        "\n",
        "###Query expansion function\n",
        "\n",
        "def query_expansion(train_df):\n",
        "\n",
        "  max_relevance_query = pd.DataFrame(train_df.groupby(['query'])['median_relevance'].max())\n",
        "  query_expanded = pd.DataFrame(columns=['query','expanded_query'])\n",
        "\n",
        "  for row in max_relevance_query.iterrows():\n",
        "    query_name , relevance = row\n",
        "    relevance = relevance.values[0]\n",
        "    query_data = pd.DataFrame(train_df[(train_df['query'] == query_name) & (train_df['median_relevance'] == relevance)][[\"query_cleaned\",\"product_title_cleaned\"]])\n",
        "    query_cleaned_ = query_data.iloc[0][0]\n",
        "    query_titles = query_data[\"product_title_cleaned\"].values.tolist()\n",
        "    query_titles_words = query_data.iloc[0][0]\n",
        "    for titles in query_titles:\n",
        "      query_titles_words.extend(titles)\n",
        "    titles_words = pd.DataFrame(query_titles_words)\n",
        "    titles_words.columns = [\"words\"]\n",
        "    count_words = pd.DataFrame(titles_words['words'].value_counts())\n",
        "    count_words[\"Rank\"] = count_words[\"words\"].rank(ascending=False)\n",
        "    count_words_subset = count_words[count_words[\"Rank\"]<=15]\n",
        "    words = list(count_words_subset.index)\n",
        "    #words = query_cleaned_\n",
        "\n",
        "    query_expand_dict = {\"query\":query_name,\"expanded_query\":words}\n",
        "    query_expanded = query_expanded.append(query_expand_dict,ignore_index=True)\n",
        "\n",
        "\n",
        "  return query_expanded\n",
        "\n",
        "### Converting data to lists as the input for the dataset class\n",
        "\n",
        "def data_format_list(train_query_expanded):\n",
        "  query_desc_rel_var = train_query_expanded[[\"expanded_query\",\"product_description_cleaned\",\"median_relevance\",\"relevance_variance\"]]\n",
        "  #query_desc_rel_var = train_query_expanded[[\"query\",\"product_description_cleaned\",\"median_relevance\",\"relevance_variance\"]]\n",
        "  query_list = query_desc_rel_var[\"expanded_query\"].values.tolist()\n",
        "  desc_list = query_desc_rel_var[\"product_description_cleaned\"].values.tolist()\n",
        "  relevance = query_desc_rel_var[\"median_relevance\"].values.tolist()\n",
        "  \n",
        "  return query_list,desc_list,relevance\n",
        "\n",
        "### Function to generate the vocabulary and word indices\n",
        "\n",
        "def get_word_to_ix(query_list,desc_list):\n",
        "  word_to_ix = {}\n",
        "  for query in query_list:\n",
        "    for token in query:\n",
        "      if token not in word_to_ix:\n",
        "        word_to_ix[token] = len(word_to_ix)\n",
        "  for desc in desc_list:\n",
        "    for token in desc:\n",
        "      if token not in word_to_ix:\n",
        "        word_to_ix[token] = len(word_to_ix)\n",
        "\n",
        "  return word_to_ix \n",
        "\n",
        "### Funtion to convert tokens to ids\n",
        "\n",
        "def get_training_data(query_list,desc_list,relevance,word_to_ix):\n",
        "  query_data = []\n",
        "  desc_data = []\n",
        "  for query in query_list:\n",
        "    query_idxs = []\n",
        "    for token in query:\n",
        "      query_idxs.extend([word_to_ix[token]])\n",
        "    query_data.append(query_idxs)\n",
        "\n",
        "  for desc in desc_list:\n",
        "    desc_idxs = []\n",
        "    for token in desc:\n",
        "      desc_idxs.extend([word_to_ix[token]])\n",
        "    desc_data.append(desc_idxs)\n",
        "\n",
        "  return query_data,desc_data,relevance\n",
        "\n",
        "## Creating a weight matrix for our vocabulary. The weights are from the w2v model\n",
        "\n",
        "def create_word2vec_weight_matrix (model,embedding_size,target_vocab):\n",
        "  weights_matrix = np.zeros((len(target_vocab)+1,embedding_size))\n",
        "  words_found = 0\n",
        "\n",
        "  for key,value in target_vocab.items():\n",
        "    try: \n",
        "      weights_matrix[value] = model.word_vec(key)\n",
        "      words_found += 1\n",
        "    except KeyError:\n",
        "      weights_matrix[value] = np.random.normal(scale=0.6, size=(embedding_size, ))  \n",
        "\n",
        "  return weights_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb8NRInOm2Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Executing the above functions\n",
        "train_df = data_cleaning(train_csv)\n",
        "test_df = data_cleaning(test_csv)\n",
        "query_expanded = query_expansion(train_df)\n",
        "train_query_expanded = train_df.merge(query_expanded,on = \"query\", how = \"left\")\n",
        "train_query_expanded = train_df\n",
        "query_list,desc_list,relevance = data_format_list(train_query_expanded)\n",
        "word_to_ix = get_word_to_ix(query_list,desc_list)\n",
        "\n",
        "###This has to be the input to the dataset class\n",
        "query_data,desc_data,relevance = get_training_data(query_list,desc_list,relevance,word_to_ix)\n",
        "\n",
        "### Creating the weight matrix\n",
        "embedding_size = 300\n",
        "weights_matrix = create_word2vec_weight_matrix(w2vmodel,embedding_size,word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQdveoDh-ejF",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and dataloader wrapping for pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aabZUMK8R43K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Wrapping the data into a dataset class, as for all pytorch models.\n",
        "\n",
        "class CrowdDataset (data.Dataset):\n",
        "\n",
        "  def __init__(self,query_list,desc_list,relevance,word_to_ix):\n",
        "    super(CrowdDataset,self).__init__()\n",
        "\n",
        "    self.query_list = query_list\n",
        "    self.desc_list = desc_list\n",
        "    self.relevance = relevance\n",
        "    self.no_of_samples = len(self.query_list)\n",
        "    self.word_to_ix = word_to_ix\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "\n",
        "    query_data = torch.LongTensor(self.query_list[index])\n",
        "    desc_data = torch.LongTensor(self.desc_list[index])\n",
        "    relevance = self.relevance[index]\n",
        "\n",
        "    return query_data,desc_data,relevance\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return self.no_of_samples\n",
        "\n",
        "def collate_fn(data):\n",
        "\n",
        "  '''\n",
        "  The batch of varying length sequences has to be padded based on the longest sentence. This funtion does that during collating the batch in the dataloader\n",
        "  It is done by using the pad_sequence function.\n",
        "  '''\n",
        "  \n",
        "  query_unsorted = [x[0] for x in data]\n",
        "  desc_unsorted = [x[1] for x in data]\n",
        "\n",
        "  query_lengths_unsorted = torch.LongTensor([len(x) for x in query_unsorted])\n",
        "  desc_lengths_unsorted = torch.LongTensor([len(x) for x in desc_unsorted])\n",
        "\n",
        "  q_lengths_sorted, q_sorted_idx = query_lengths_unsorted.sort(descending=True)\n",
        "  d_lengths_sorted, d_sorted_idx = desc_lengths_unsorted.sort(descending=True)\n",
        "  \n",
        "  sorted_batch_query = sorted(data, key=lambda x: x[0].shape[0], reverse=True)\n",
        "  query_sorted = [x[0] for x in sorted_batch_query]\n",
        "  query_padded = torch.nn.utils.rnn.pad_sequence(query_sorted, batch_first=True,padding_value = 27370)\n",
        "  query_lengths = torch.LongTensor([len(x) for x in query_sorted])\n",
        "\n",
        "  sorted_batch_desc = sorted(data, key=lambda x: x[1].shape[0], reverse=True)\n",
        "  desc_sorted = [x[1] for x in sorted_batch_desc]\n",
        "  desc_padded = torch.nn.utils.rnn.pad_sequence(desc_sorted, batch_first=True,padding_value = 27370)\n",
        "  desc_lengths = torch.LongTensor([len(x) for x in desc_sorted])\n",
        "\n",
        "  labels = [(x[2]-1) for x in data]\n",
        "  \n",
        "  return query_padded,desc_padded,query_lengths,desc_lengths,q_sorted_idx,d_sorted_idx,torch.tensor(labels)\n",
        "\n",
        "###Creating validation data\n",
        "\n",
        "## Sampling data for train and validation\n",
        "\n",
        "def val_sampler (dataset,validation_split):\n",
        "\n",
        "  dataset_len = len(dataset)\n",
        "  indices = list(range(dataset_len))\n",
        "\n",
        "  val_len = int(np.floor(validation_split * dataset_len))\n",
        "  validation_idx = np.random.choice(indices, size=val_len, replace=False)\n",
        "  train_idx = list(set(indices) - set(validation_idx))\n",
        "\n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  val_sampler = SubsetRandomSampler(validation_idx)\n",
        "\n",
        "  train_actual_classes = [relevance[i] for i in train_sampler]\n",
        "  val_actual_classes = [relevance[i] for i in val_sampler]\n",
        "\n",
        "  return train_sampler,val_sampler,len(train_idx),val_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSx2Qz9TpCf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Calling the dataset class and the validation sampler function\n",
        "\n",
        "dataset = CrowdDataset(query_data,desc_data,relevance,word_to_ix)\n",
        "train_sampler,val_sampler,train_len,val_len = val_sampler(dataset,0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVnYtGL_Wj4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Creating the dataloader wrapper to batch the data\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = dataset,batch_size= 50,sampler= train_sampler,collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(dataset = dataset,batch_size= 50,sampler= val_sampler,collate_fn=collate_fn)\n",
        "\n",
        "data_loaders = {\"train\": train_loader, \"val\": val_loader}\n",
        "data_lengths = {\"train\": train_len, \"val\": val_len}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ych9Lmg9_CjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Function to create the embedding layer using the weight matric created before\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape\n",
        "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "    if non_trainable:\n",
        "      emb_layer.weight.requires_grad = False\n",
        "    else:\n",
        "      emb_layer.weight.requires_grad = True\n",
        "    return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecx1Mmdi-kIh",
        "colab_type": "text"
      },
      "source": [
        "# Model architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez8svm9qqtbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The model architecture is very similar to a siamese net. there are two LSTM heads, one for the query and one for the description. These LSTM heads are independent.\n",
        "The output from these LSTM heads are then concatenated into a final feature vector and is directly used for the final fc layer and class probability calculation\n",
        "\n",
        "The entire architecture is divided into two classes: LSTMencoder and Siamese_lstm.\n",
        "\n",
        "'''\n",
        "\n",
        "class LSTMencoder (nn.Module):\n",
        "\n",
        "  def __init__(self, weights_matrix, hidden_dim_q_1,hidden_dim_q_2,hidden_dim_d_1,hidden_dim_d_2,device):\n",
        "    super(LSTMencoder,self).__init__()\n",
        "\n",
        "    self.embedding_token, num_embeddings, embedding_dim = create_emb_layer(weights_matrix)\n",
        "\n",
        "    self.lstm_q_1 = nn.GRU(embedding_dim, hidden_dim_q_1,batch_first = True)\n",
        "    self.lstm_q_2 = nn.LSTM(hidden_dim_q_1,hidden_dim_q_2,batch_first = True)\n",
        "\n",
        "    self.lstm_d_1 = nn.GRU(embedding_dim, hidden_dim_d_1,batch_first = True)\n",
        "    self.lstm_d_2 = nn.LSTM(hidden_dim_d_1,hidden_dim_d_2,batch_first = True)\n",
        "\n",
        "    self.final_hidden_q = hidden_dim_q_2\n",
        "    self.final_hidden_d = hidden_dim_d_2\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,query_batch,desc_batch,q_lengths,d_lengths,q_ix,d_ix):\n",
        "\n",
        "    query_embedding = self.embedding_token(query_batch)\n",
        "    desc_embedding = self.embedding_token(desc_batch)\n",
        "\n",
        "    q_packed = PACK(query_embedding,q_lengths, batch_first=True)\n",
        "    d_packed = PACK(desc_embedding,d_lengths,batch_first=True)\n",
        "\n",
        "    output_q1, _ = self.lstm_q_1(q_packed)\n",
        "    output_q2, _ = self.lstm_q_2(output_q1)\n",
        "\n",
        "    output_d1, _ = self.lstm_d_1(d_packed)\n",
        "    output_d2, _ = self.lstm_d_2(output_d1)\n",
        " \n",
        "    output_q_padded, output_q_lengths = pad_packed_sequence(output_q2, batch_first=True)\n",
        "    output_d_padded, output_d_lengths = pad_packed_sequence(output_d2, batch_first=True)\n",
        "\n",
        "    \n",
        "    idx_q = (torch.LongTensor(q_lengths) - 1).view(-1, 1).expand(len(q_lengths), output_q_padded.size(2))\n",
        "    idx_d = (torch.LongTensor(d_lengths) - 1).view(-1, 1).expand(len(d_lengths), output_d_padded.size(2))\n",
        "\n",
        "    '''\n",
        "    The feature vector from the final time step of the sequence has to be extracted. This woould be the feature vector for the sequence\n",
        "\n",
        "    '''  \n",
        "    time_dimension = 1\n",
        "    idx_q = idx_q.unsqueeze(time_dimension)\n",
        "    idx_d = idx_d.unsqueeze(time_dimension)\n",
        "    if (output_q_padded.is_cuda or output_d_padded.is_cuda):\n",
        "      idx_q = idx_q.cuda(output_q_padded.data.get_device())\n",
        "      idx_d = idx_d.cuda(output_q_padded.data.get_device())\n",
        "    \n",
        "    last_output_q = output_q_padded.gather(time_dimension, Variable(idx_q)).squeeze(time_dimension)\n",
        "    last_output_d = output_d_padded.gather(time_dimension, Variable(idx_d)).squeeze(time_dimension)\n",
        "\n",
        "    last_output_q = last_output_q.to(self.device)\n",
        "    last_output_d = last_output_d.to(self.device)\n",
        "\n",
        "    '''\n",
        "    For padding the queries and desc batches, both the query and description batches were sorted. \n",
        "    Now before the final calculation we will have to re sort it to make sure the query and desc are matched as inputs\n",
        "\n",
        "    '''\n",
        "    \n",
        "    last_output_query = torch.zeros_like(last_output_q).scatter_(0, Variable(q_ix).unsqueeze(1).expand(-1, last_output_q.shape[1]), last_output_q)\n",
        "    last_output_desc = torch.zeros_like(last_output_d).scatter_(0, Variable(d_ix).unsqueeze(1).expand(-1, last_output_d.shape[1]), last_output_d)\n",
        "\n",
        "    return last_output_q,last_output_d,last_output_query,last_output_desc\n",
        "\n",
        "class Siamese_lstm(nn.Module):\n",
        "\n",
        "  def __init__(self, weights_matrix, hidden_dim_q_1,hidden_dim_q_2,hidden_dim_d_1,hidden_dim_d_2,label_size,device):\n",
        "    super(Siamese_lstm, self).__init__()\n",
        "    self.encoder = LSTMencoder(weights_matrix,hidden_dim_q_1,hidden_dim_q_2,hidden_dim_d_1,hidden_dim_d_2,device)\n",
        "\n",
        "    self.feature_len = 2*self.encoder.final_hidden_q\n",
        "    self.feature_len2 = 32\n",
        "    self.final_layer = nn.Linear(self.feature_len,self.feature_len2)\n",
        "    self.final_layer_1 = nn.Linear(self.feature_len2,label_size)\n",
        "\n",
        "  def forward(self,query_batch,desc_batch,q_lengths,d_lengths,q_ix,d_ix):\n",
        "\n",
        "    _,_,query_feature,desc_feature = self.encoder(query_batch,desc_batch,q_lengths,d_lengths,q_ix,d_ix)\n",
        "\n",
        "    final_vector = torch.cat((query_feature,desc_feature), 1)\n",
        "    #final_vector = query_feature*desc_feature\n",
        "\n",
        "    \n",
        "    output = self.final_layer(final_vector)\n",
        "    output_1 = self.final_layer_1(output)\n",
        "\n",
        "    return output_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH_T_omK-o6L",
        "colab_type": "text"
      },
      "source": [
        "# Model parameters and running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2YHdhrqFPga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Siamese_lstm(weights_matrix,32,64,32,64,4,device)\n",
        "model = model.to(device) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNFOkVdHhxKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Giving the loss function, optimizer and the scheduler\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.CyclicLR(optimizer_ft, base_lr = 0.000001 ,max_lr = 0.01)\n",
        "\n",
        "num_epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz5WXiATeGNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Function to train the model\n",
        "\n",
        "def train_model(model,criterion,optimizer,scheduler,num_epochs):\n",
        "    \n",
        "  since = time.time()\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "      \n",
        "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "    print('-' * 10)\n",
        "      \n",
        "    for phase in ['train','val']:\n",
        "          \n",
        "      if (phase == 'train'):\n",
        "        #scheduler.step()\n",
        "        model.train()\n",
        "\n",
        "      else:\n",
        "        model.eval()\n",
        "              \n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0.0\n",
        "\n",
        "      ##Iterate over data\n",
        "          \n",
        "      for query_batch,desc_batch,query_lengths,desc_lengths,q_sorted_idx,d_sorted_idx,labels in data_loaders[phase]:\n",
        "        \n",
        "        query_batch = query_batch.to(device)\n",
        "        desc_batch = desc_batch.to(device)\n",
        "        labels = labels.to(device)\n",
        "        q_sorted_idx = q_sorted_idx.to(device)\n",
        "        d_sorted_idx = d_sorted_idx.to(device)\n",
        "              \n",
        "        optimizer.zero_grad()\n",
        "              \n",
        "        with torch.set_grad_enabled(phase=='train'):\n",
        "          outputs = model(query_batch,desc_batch,query_lengths,desc_lengths,q_sorted_idx,d_sorted_idx)\n",
        "          _,preds = torch.max(outputs,1)\n",
        "          \n",
        "          loss = criterion(outputs,labels)\n",
        "          \n",
        "          if(phase=='train'):\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()          \n",
        "        ##Statistics\n",
        "              \n",
        "        running_loss += loss.item() * query_batch.size(0)\n",
        "        running_corrects += torch.sum(preds == labels) \n",
        "\n",
        "      epoch_loss = running_loss / data_lengths[phase]\n",
        "      epoch_acc = running_corrects.double() / data_lengths[phase]\n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "      \n",
        "      if (phase == 'train'):\n",
        "        scheduler.step() \n",
        "      \n",
        "      if phase == 'val' and epoch_acc > best_acc:\n",
        "          best_acc = epoch_acc\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                  \n",
        "    print()\n",
        "      \n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best Accuracy: {:4f}'.format(best_acc))\n",
        "      \n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxx--7qCe0Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Calling the model function\n",
        "model_1 = train_model(model,criterion,optimizer_ft,exp_lr_scheduler,num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}